import requests
import pandas as pd
import numpy as np
import json
import webbrowser
import os
import re
import time
import threading
from tqdm import tqdm
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from concurrent.futures import ThreadPoolExecutor, as_completed
from selenium.webdriver.support import expected_conditions as EC


# Set display options for better spacing
pd.set_option('display.max_columns', None)        # show all columns
pd.set_option('display.width', 1000)              # set max line width
pd.set_option('display.max_colwidth', None)       # don't truncate cell values
pd.set_option('display.unicode.east_asian_width', True)  # better for Korean spacing

from config import EXCEL_FILES, COMBINED_EXCEL_OUTPUT, COMBINED_CSV_OUTPUT
from config import TARGET_SIDO_CODES, BASE_GUNGU_URL, BASE_DONG_URL, BASE_APT_URL, BASE_HEADERS, BASE_SIDO_URL

'''[NAVER] ë„¤ì´ë²„ ë¶€ë™ì‚°ì—ì„œ ëª¨ë“  markerid ê°€ì ¸ì˜¤ê¸°'''
def get_sido_info():
    """Fetch list of ì‹œë„ (provinces) with codes and names."""
    r = requests.get(BASE_SIDO_URL, data={"sameAddressGroup": "false"}, headers=BASE_HEADERS)
    r.encoding = "utf-8-sig"
    temp = json.loads(r.text)
    return temp["regionList"] 
def get_gungu_info(sido_code):
    r = requests.get(BASE_GUNGU_URL + sido_code, data={"sameAddressGroup": "false"}, headers=BASE_HEADERS)
    r.encoding = "utf-8-sig"
    temp = json.loads(r.text)
    return temp["regionList"]
def get_dong_info(gungu_code):
    r = requests.get(BASE_DONG_URL + gungu_code, data={"sameAddressGroup": "false"}, headers=BASE_HEADERS)
    r.encoding = "utf-8-sig"
    temp = json.loads(r.text)
    return temp["regionList"]
def get_apt_list(dong_code):
    r = requests.get(BASE_APT_URL + dong_code + '&realEstateType=APT&order=', data={"sameAddressGroup": "false"}, headers=BASE_HEADERS)
    r.encoding = "utf-8-sig"
    temp = json.loads(r.text)
    try:
        return temp['complexList']
    except:
        return []
#ê° ì‹œì— ëŒ€í•œ ê° êµ¬ì— ëŒ€í•œ ê° ë™ì— ëŒ€í•œ ê° ì•„íŒŒíŠ¸ ì½”ë“œë¥¼ ê°€ì ¸ì™€ì„œ ì´í•©í•˜ê¸° df
def make_df():
    sido_list = get_sido_info()
    results = []
    
    for sido in sido_list:
        if sido['cortarNo'] not in TARGET_SIDO_CODES:
            continue

        gungu_list = get_gungu_info(sido['cortarNo'])
        for gungu in tqdm(gungu_list):
            dong_list = get_dong_info(gungu['cortarNo'])
            for dong in dong_list:
                apt_list = get_apt_list(dong['cortarNo'])
                for apt in apt_list:
                    print(
                        apt['complexNo'],     # ì•„íŒŒíŠ¸ primary key
                        sido['cortarName'],   # ì‹œë„ name
                        gungu['cortarName'],  # êµ°êµ¬ name
                        dong['cortarName'],   # ë™ name
                        apt['complexName']    # ì•„íŒŒíŠ¸ name
                    )
                    results.append({
                        'complexNo': apt['complexNo'], # markerid - ê³ ìœ í‚¤
                        'sido': sido['cortarName'],
                        'gungu': gungu['cortarName'],
                        'dong': dong['cortarName'],
                        'complexName': apt['complexName']
                    })
    df = pd.DataFrame(results)
    return df


'''ì—‘ì…€ íŒŒì¼ë“¤ì„ í•˜ë‚˜ì˜ dfë¡œ í•©ì³ì£¼ê¸° -- configì—ì„œ MOLIT ì—‘ì…€ ìë£Œ í•©ì¹˜ê¸° ìœ„í•¨'''    
def combine_excel(file_list):
    """Combine multiple Excel (.xlsx) files into one DataFrame with new index."""
    df_list = []
    for file in file_list:
        try:
            df = pd.read_excel(file)  # Excel doesnâ€™t need encoding
            df_list.append(df)
            print(f"âœ… Loaded: {file} ({len(df)} rows)")
        except Exception as e:
            print(f"âŒ Failed to load {file}: {e}")
    
    if df_list:
        combined_df = pd.concat(df_list, ignore_index=True)
        combined_df.reset_index(drop=True, inplace=True)
        return combined_df
    else:
        print("âš ï¸ No valid Excel files found.")
        return pd.DataFrame()

'''df í¸í•˜ê²Œ ë³´ëŠ” ë°©ë²• í•¨ìˆ˜'''
def preview(df, filename="df_preview.html"):
    style = """
    <style>
        table {
            border-collapse: collapse;
            width: 100%;
        }
        th, td {
            white-space: nowrap;
            padding: 6px 10px;
            border: 1px solid #ccc;
            text-align: center;
        }
        thead {
            background-color: #f2f2f2;
            position: sticky;
            top: 0;
        }
        .scroll-container {
            overflow-x: auto;
            width: 100%;
        }
        body {
            font-family: sans-serif;
        }
    </style>
    """

    html = df.to_html(index=False, escape=False)
    full_html = f"{style}<div class='scroll-container'>{html}</div>"

    with open(filename, "w", encoding="utf-8") as f:
        f.write(full_html)

    webbrowser.open("file://" + os.path.abspath(filename))
    print(f"âœ… Scrollable preview opened: {os.path.abspath(filename)}")

# =================================================================================================
'''Storing and Loading functions'''
def load_step(n):
    path = f"res csv/step_{n}.csv"
    if not os.path.exists(path):
        raise FileNotFoundError(f"step {n} result not found at {path}")
    return pd.read_csv(path) 
def load_csv(file_name):
    path = f"res csv/{file_name}.csv"
    if not os.path.exists(path):
        raise FileNotFoundError(f"{file_name} result not found at {path}")
    return pd.read_csv(path)

def store_result(df, file_name):
    path = f"res csv/{file_name}.csv"
    if not os.path.exists(path):
        df.to_csv(path, index=False)
        print(f"âœ… Saved {file_name} result to {path}")
    else:
        print(f"ğŸ“‚ Step {file_name} already exists: {path}")

#==================================================================================================
'''ê°ì¢… Tools. part of preprocessing'''
def unique_df(df, key_column):
    """
    [ROLE] Keep only rows with unique values in `key_column`.

    Parameters:
    - df: a DataFrame or string name of a CSV in 'res csv/'
    - key_column: column to check uniqueness on

    Returns:
    - DataFrame with only the first occurrence of each unique key_column value
    """
    print(f"{"Original data shape: ", df.shape}")
    df = load_csv(df) if isinstance(df, str) else df
    # Drop duplicates based on key_column, keeping the first occurrence
    df_unique = df.drop_duplicates(subset=key_column, keep="first").reset_index(drop=True)
    print(f"{"Unique data shape: ", df_unique.shape}" )
    
    
    return df_unique
    
def mapping(source_file, source_column, target_file, target_column, call_column, insert_at, new_col_name):
    '''[ROLE] source_fileì˜ source_column ê°’ì„ target_fileì˜ target_columnì—ì„œ ì°¾ì•„ë³´ê³  call_columnê°€ì ¸ì˜¤ê¸°
        Parameters:
    - source_file: str(csv) or df
    - source_column: column to match from source_file
    - target_file: str(csv) or df
    - target_column: column to match from target_file
    - call_column: column to retrieve from target_file
    - insert_at: 'first', 'last', or (int) index to insert the new column after (int)th column
    - new_col_name: name for the new column in source_df'''
    

    # Load if CSV, else assume already DataFrame
    source_df = load_csv(source_file) if isinstance(source_file, str) else source_file
    target_df = load_csv(target_file) if isinstance(target_file, str) else target_file
    
    # Build mapping dictionary
    mapping_dict = dict(zip(target_df[target_column], target_df[call_column]))
    # Perform mapping
    mapped_series = source_df[source_column].map(mapping_dict)
    
    # Log mapping success and fill unmapped
    total = len(mapped_series)
    mapped = mapped_series.notna().sum()
    unmapped = total - mapped
    print(f"âœ… {mapped}/{total} values successfully mapped ({unmapped} unmapped).")

    mapped_series = mapped_series.fillna("UNMAPPED")
    
    # Determine insert index
    if insert_at == "first":
        insert_idx = 0
    elif insert_at == "last":
        insert_idx = len(source_df.columns)
    elif isinstance(insert_at, int):
        insert_idx = min(insert_at + 1, len(source_df.columns))
    else:
        raise ValueError("insert_at must be 'first', 'last', or an integer")

    # Insert new column
    source_df.insert(insert_idx, new_col_name, mapped_series)

    return source_df

def update_key(df):
    """
    [ROLE] Create or update [KEY]markerid column at index 0.
    Ensures most recent [P#]markerid columns are ordered by descending step (P10, P7, P5, etc.).
    """
    # 1. Find all [P#]markerid columns (e.g. [P10]markerid)
    candidate_cols = [col for col in df.columns if re.match(r"\[P\d+\]markerid", col)]

    if not candidate_cols:
        raise ValueError("âŒ No valid [P#]markerid columns found in DataFrame.")

    # 2. Sort them in descending P# order (most recent first)
    candidate_cols_sorted = sorted(
        candidate_cols, key=lambda c: int(re.findall(r"\[P(\d+)\]", c)[0]), reverse=True
    )

    # 3. Generate unified markerid
    final_markerid = df[candidate_cols_sorted].apply(
        lambda row: next((val for val in row if val != "UNMAPPED"), "UNMAPPED"), axis=1
    )

    # 4. Remove existing [KEY]markerid if present
    if "[KEY]markerid" in df.columns:
        df.drop(columns=["[KEY]markerid"], inplace=True)

    # 5. Build final column order
    remaining_cols = [col for col in df.columns if col not in candidate_cols_sorted]
    new_col_order = candidate_cols_sorted + remaining_cols  # no [KEY] yet

    df = df[new_col_order]  # reorder
    df.insert(0, "[KEY]markerid", final_markerid)  # insert at front

    # 6. Log
    total = len(final_markerid)
    success = (final_markerid != "UNMAPPED").sum()
    print(f"âœ… [KEY]markerid: {success}/{total} mapped successfully.")
    print(f"   Used columns (ordered): {candidate_cols_sorted}")

    return df
    
def col_type(df, col_name, target_type):
    """
    [ROLE] Convert a specific column in a df to a given type.
    Supported types: 'int', 'float', 'str'

    Example:
        df = col_type(df, '[P10]markerid', 'int')
    """
    if col_name not in df.columns:
        raise ValueError(f"âŒ Column '{col_name}' not found in DataFrame.")

    if target_type == "int":
        df[col_name] = df[col_name].apply(
            lambda x: int(float(x)) if pd.notna(x) and str(x).replace('.', '', 1).isdigit() else x
        )
    elif target_type == "float":
        df[col_name] = df[col_name].apply(
            lambda x: float(x) if pd.notna(x) and str(x).replace('.', '', 1).replace('-', '', 1).replace(',', '', 1).replace(' ', '') != "" else x
        )
    elif target_type == "str":
        df[col_name] = df[col_name].astype(str)
    else:
        raise ValueError(f"âŒ Unsupported target type: {target_type}")

    print(f"ğŸ”§ column '{col_name}' to type '{target_type}' âœ…")
    return df

def update_key(df): # [MOLIT] step_5.csvì— 1ì—´ All_markerid ì—´ ì¶”ê°€í•˜ê¸°
    """
    [ROLE] Create or update [KEY]markerid column at index 0.
    Ensures most recent [P#]markerid columns are ordered by descending step (P10, P7, P5, etc.).
    """
    # 1. Find all [P#]markerid columns (e.g. [P10]markerid)
    candidate_cols = [col for col in df.columns if re.match(r"\[P\d+\]markerid", col)]

    if not candidate_cols:
        raise ValueError("âŒ No valid [P#]markerid columns found in DataFrame.")

    # 2. Sort them in descending P# order (most recent first)
    candidate_cols_sorted = sorted(
        candidate_cols, key=lambda c: int(re.findall(r"\[P(\d+)\]", c)[0]), reverse=True
    )

    # 3. Generate unified markerid
    final_markerid = df[candidate_cols_sorted].apply(
        lambda row: next((val for val in row if val != "UNMAPPED"), "UNMAPPED"), axis=1
    )

    # 4. Remove existing [KEY]markerid if present
    if "[KEY]markerid" in df.columns:
        df.drop(columns=["[KEY]markerid"], inplace=True)

    # 5. Build final column order
    remaining_cols = [col for col in df.columns if col not in candidate_cols_sorted]
    new_col_order = candidate_cols_sorted + remaining_cols  # no [KEY] yet

    df = df[new_col_order]  # reorder
    df.insert(0, "[KEY]markerid", final_markerid)  # insert at front

    # 6. Log
    total = len(final_markerid)
    success = (final_markerid != "UNMAPPED").sum()
    print(f"âœ… [KEY]markerid: {success}/{total} mapped successfully.")
    print(f"   Used columns (ordered): {candidate_cols_sorted}")

    return df


# Thread-local storage to keep one driver per thread
thread_local = threading.local()

def get_driver():
    if not hasattr(thread_local, "driver"):
        options = Options()
        options.add_argument("--headless")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--log-level=3")
        thread_local.driver = webdriver.Chrome(options=options)
    return thread_local.driver

def crawl(df, source_col, new_col_name, max_workers=5):
    target_rows = df[df["[KEY]markerid"] == "UNMAPPED"].copy()
    print(f"ğŸ” Crawling markerIds for {len(target_rows)} unmapped entries...")

    results = {}

    def process_row(index, search_term):
        driver = get_driver()
        result = "UNMAPPED"
        try:
            driver.get("https://m.land.naver.com/search")
            WebDriverWait(driver, 4).until(EC.presence_of_element_located((By.ID, "query")))
            search_box = driver.find_element(By.ID, "query")
            search_box.clear()
            search_box.send_keys(search_term)
            search_box.send_keys(Keys.ENTER)
            time.sleep(2.5)

            url = driver.current_url
            match = re.search(r"/complexes/(\d+)", url)
            if match:
                result = match.group(1)
                print(f"âœ… {search_term} â†’ {result}")
            else:
                print(f"âŒ {search_term} â†’ MarkerID not found")
        except Exception as e:
            print(f"ğŸš¨ Error for '{search_term}': {e}")
        return index, result

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(process_row, idx, row[source_col]): idx
            for idx, row in target_rows.iterrows()
        }

        for future in tqdm(as_completed(futures), total=len(futures), desc="ğŸ”„ Crawling"):
            idx, markerid = future.result()
            results[idx] = markerid

    # Update results into DataFrame
    df[new_col_name] = "UNMAPPED"
    for idx, markerid in results.items():
        df.at[idx, new_col_name] = markerid

    # Move the new column next to [KEY]markerid
    if new_col_name in df.columns:
        df = df.drop(columns=[new_col_name])
    insert_idx = df.columns.get_loc("[KEY]markerid") + 1
    df.insert(insert_idx, new_col_name, [results.get(i, "UNMAPPED") for i in df.index])

    print(f"ğŸŸ¢ Finished crawling. {sum(v != 'UNMAPPED' for v in results.values())} / {len(results)} mapped.")
    return df

# =================================================================================================
'''[MOLIT] Preprocessing functions'''
# ê° ë‹¨ê³„ì˜ ê²°ê³¼ëŠ” res fileì— ì €ì¥ë¨. ë‹¤ìŒ ë‹¨ê³„ì—ì„œëŠ” ì´ì „ ë‹¨ê³„ ê²°ê³¼ íŒŒì¼ ë¶ˆëŸ¬ì™€ì„œ ì‘ë™.
"""Load the result of step n from res csv/step_n.csv"""
# ì…ë ¥: step n # ì¶œë ¥: step_n.csvë¥¼ dfë¡œ return

def preprocess_1(df): # [MOLIT] ê´‘ì—­ì‹œ --> ì‹œ
    '''[ROLE] **ê´‘ì—­ì‹œ --> **ì‹œ ë¡œ ë°”ê¾¸ëŠ” ì‘ì—… & **ì‹œ **êµ¬ **ë™ --> ê°ì—´ë¡œ ì°¢ê¸°'''
    
    
    '''1. **ê´‘ì—­ì‹œ --> **ì‹œ ë¡œ ë°”ê¾¸ëŠ” ì‘ì—…'''
    #res csv/step_0.csv --> 2ì—´(ì‹œêµ°êµ¬)
    city_replacements = {
        "ê´‘ì£¼ê´‘ì—­ì‹œ": "ê´‘ì£¼ì‹œ",
        "ëŒ€ì „ê´‘ì—­ì‹œ": "ëŒ€ì „ì‹œ",
        "ëŒ€êµ¬ê´‘ì—­ì‹œ": "ëŒ€êµ¬ì‹œ",
        "ë¶€ì‚°ê´‘ì—­ì‹œ": "ë¶€ì‚°ì‹œ",
    }
    def replace_city_name(value):
        for old, new in city_replacements.items():
            if value.startswith(old):
                return value.replace(old, new, 1)
        return value

    df["ì‹œêµ°êµ¬"] = df["ì‹œêµ°êµ¬"].apply(replace_city_name)
    
  
    '''2. **ì‹œ **êµ¬ **ë™ --> ê°ì—´ë¡œ ì°¢ê¸°'''
    if "ì‹œêµ°êµ¬" not in df.columns:
        raise ValueError("âŒ 'ì‹œêµ°êµ¬' column not found in the DataFrame.")
    # Find index of ì‹œêµ°êµ¬ column
    
    # Split 'ì‹œêµ°êµ¬' into 3 parts: col names: **ì‹œ, **êµ¬, **ë™
    split_cols = df["ì‹œêµ°êµ¬"].str.split(" ", n=2, expand=True)
    split_cols.columns = ["**ì‹œ", "**êµ¬", "**ë™"]
    insert_idx = df.columns.get_loc("ì‹œêµ°êµ¬") + 1
    
    for i, col_name in enumerate(split_cols.columns):
        df.insert(insert_idx + i, col_name, split_cols[col_name])
    
    return df

def preprocess_2(df): # [MOLIT] ì‹œêµ°êµ¬ë‹¨ì§€ëª… col ë§Œë“¤ê¸°
    '''[ROLE] make ì‹œêµ°êµ¬ + ë‹¨ì§€ëª… into 1 column. Clean '**ë™' if needed, then insert after '**ë™'.'''

    if "**ë™" not in df.columns:
        raise ValueError("âŒ '**ë™' column not found in DataFrame.")
    
    # Step 1: Clean '**ë™' values (keep only the first word if there's a space)
    df["**ë™"] = df["**ë™"].astype(str).str.split().str[0]

    # Step 2: Combine ì‹œ + êµ¬ + cleaned ë™ + ë‹¨ì§€ëª…
    combined = (
        df["**ì‹œ"].astype(str) + " " +
        df["**êµ¬"].astype(str) + " " +
        df["**ë™"].astype(str) + " " +
        df["ë‹¨ì§€ëª…"].astype(str)
    )

    # Step 3: Insert new column after '**ë™'
    insert_idx = df.columns.get_loc("**ë™") + 1
    df.insert(insert_idx, "[P2]ì‹œêµ°êµ¬_ë‹¨ì§€ëª…", combined)

    return df

def preprocess_3(df): #[MOLIT] Unique
    '''[ROLE] UNIQUEí•œ ì‹œêµ°êµ¬ ë‹¨ì§€ëª…ë§Œ ë‚¨ê¸°ê¸° from MOLIT = step_2'''
    df3 = unique_df(df, "[P2]ì‹œêµ°êµ¬_ë‹¨ì§€ëª…")
    return df3

def preprocess_4(df): # [NAVER markerid] ì‹œêµ°êµ¬ë‹¨ì§€ëª… col ë§Œë“¤ê¸°
    '''[ROLE] make ì‹œêµ°êµ¬ ë‹¨ì§€ëª… into 1 column. insert after complexName column'''    
    # Step 1: Create new column
    combined = df["sido"].astype(str) + " " + df["gungu"].astype(str) + " " + df["dong"].astype(str) +" " + df["complexName"].astype(str)
    if "complexName" not in df.columns:
        raise ValueError("âŒ 'complexName' column not found in DataFrame.")
    
    # Step 2: Insert index
    insert_idx = df.columns.get_loc("complexName") + 1  # insert AFTER 'complexName'
    # Step 3: Insert new column
    df.insert(insert_idx, "[P4]ì‹œêµ°êµ¬_ë‹¨ì§€ëª…", combined)

    return df
    
def preprocess_5(df): # [mapping] 1
    return mapping(
    source_file=df,
    source_column="[P2]ì‹œêµ°êµ¬_ë‹¨ì§€ëª…",
    target_file="markerid_1",
    target_column="[P4]ì‹œêµ°êµ¬_ë‹¨ì§€ëª…",
    call_column="complexNo",
    insert_at="first",
    new_col_name="[P5]markerid"
    )   
    
def preprocess_6(df): # [NAVER markerid] "(ì£¼ìƒë³µí•©)", "(ë„ì‹œí˜•)" ì§€ìš°ê¸°
    '''[ROLE] df(markerid)remove "(ì£¼ìƒë³µí•©)", "(ë„ì‹œí˜•)" from complexName'''
    
    col_to_clean = "[P4]ì‹œêµ°êµ¬_ë‹¨ì§€ëª…"
    new_col_name = "[P6]ì‹œêµ°êµ¬_ë‹¨ì§€ëª…_cleaned_(ì£¼ìƒë³µí•©)(ë„ì‹œí˜•)"

    if col_to_clean not in df.columns:
        raise ValueError(f"âŒ '{col_to_clean}' column not found in DataFrame.")

    # Step 1: Clean the target column
    cleaned = (
        df[col_to_clean]
        .astype(str)
        .str.replace("(ì£¼ìƒë³µí•©)", "", regex=False)
        .str.replace("(ë„ì‹œí˜•)", "", regex=False)
        .str.strip()
    )

    # Step 2: Insert cleaned version right after the original column
    insert_idx = df.columns.get_loc(col_to_clean) + 1
    df.insert(insert_idx, new_col_name, cleaned)

    return df
    
def preprocess_7(df): # [mapping] 2
    return mapping(
    source_file=df,
    source_column="[P2]ì‹œêµ°êµ¬_ë‹¨ì§€ëª…",
    target_file="markerid_2",
    target_column="[P6]ì‹œêµ°êµ¬_ë‹¨ì§€ëª…_cleaned_(ì£¼ìƒë³µí•©)(ë„ì‹œí˜•)",
    call_column="complexNo",
    insert_at="first",
    new_col_name="[P7]markerid"
    )

def preprocess_8(df): # [MOLIT] 1ì—´ All_markerid ì—´ ì¶”ê°€í•˜ê¸°
    return update_key(df)

def preprocess_9(df): # [MOLIT] ë‹¨ì§€ëª…ì—ì„œ '**ë™' prefix ì§€ìš°ê¸°
    '''[ROLE] If ë‹¨ì§€ëª… starts with '**ë™', remove that prefix and insert cleaned version after [P2]ì‹œêµ°êµ¬_ë‹¨ì§€ëª….'''
    if "**ë™" not in df.columns or "ë‹¨ì§€ëª…" not in df.columns or "[P2]ì‹œêµ°êµ¬_ë‹¨ì§€ëª…" not in df.columns:
        raise ValueError("âŒ Missing one or more required columns: '**ë™', 'ë‹¨ì§€ëª…', '[P2]ì‹œêµ°êµ¬_ë‹¨ì§€ëª…'.")

    # Step 1: Remove prefix from ë‹¨ì§€ëª… if it matches **ë™
    cleaned = df.apply(
        lambda row: row["ë‹¨ì§€ëª…"][len(row["**ë™"]):] if str(row["ë‹¨ì§€ëª…"]).startswith(str(row["**ë™"])) else row["ë‹¨ì§€ëª…"],
        axis=1
    )

    # Step 2: Insert new column after [P2]ì‹œêµ°êµ¬_ë‹¨ì§€ëª…
    insert_idx = df.columns.get_loc("[P2]ì‹œêµ°êµ¬_ë‹¨ì§€ëª…") + 1
    df.insert(insert_idx, "[P9]ë‹¨ì§€ëª…_erased_**ë™", cleaned)

    return df

def preprocess_10(df): # [mapping] 3
    '''[ROLE] step_6ì˜ 'ë‹¨ì§€ëª…' prefixì—ì„œ '**ë™' ì§€ìš´ê±° mapping í•˜ê¸°.'''
    return mapping(
    source_file=df,
    source_column="[P9]ë‹¨ì§€ëª…_erased_**ë™",
    target_file="markerid_2",
    target_column="[P6]ì‹œêµ°êµ¬_ë‹¨ì§€ëª…_cleaned_(ì£¼ìƒë³µí•©)(ë„ì‹œí˜•)",
    call_column="complexNo",
    insert_at=2,
    new_col_name="[P10]markerid"
    )

def preprocess_11(df): # [MOLIT] markerid ì—´ë“¤ì„ ì†Œìˆ˜ì ì—ì„œ int ë¡œ ë°”ê¿”ì£¼ê¸°
    df = col_type(df, "[KEY]markerid", "int")
    df = col_type(df, "[P10]markerid", "int")
    df = col_type(df, "[P7]markerid", "int")
    df = col_type(df, "[P5]markerid", "int")
    return df

def preprocess_12(df): # [MOLIT] "**ì‹œ + **êµ¬ + '[P9]ë‹¨ì§€ëª…_erased_**ë™' " ì—´ ë§Œë“¤ê¸°. ê²€ìƒ‰ì¤€ë¹„ì‹œì¼œì£¼ê¸°
    '''ì›¹í¬ë¡¤ë§ ì¤€ë¹„ì‹œì¼œì£¼ê¸° ì‘ì—…. ê´‘ì£¼ì‹œ ë¶êµ¬ ìš´ì•”ë™ ìš´ì•”1ì°¨ë‚¨ì–‘íœ´íŠ¼ --> ê´‘ì£¼ ë¶êµ¬ ìš´ì•”1ì°¨ë‚¨ì–‘íœ´íŠ¼
        "**ë™" ì§€ìš°ê¸° (ë™ì„ ì§€ìš°ê³  ë„¤ì´ë²„ì— ê²€ìƒ‰í•´ì•¼ì§€ í¬ë¡¤ë§ì— ì˜ ë‚˜ì˜´.)
    '''
    # Combine ì‹œ + êµ¬ + ë‹¨ì§€ëª…
    combined = (
        df["**ì‹œ"].astype(str) + " " +
        df["**êµ¬"].astype(str) + " " +
        df["[P9]ë‹¨ì§€ëª…_erased_**ë™"].astype(str)
    )

    # Insert new column after '**ë™'
    insert_idx = df.columns.get_loc("[P9]ë‹¨ì§€ëª…_erased_**ë™") + 1
    df.insert(insert_idx, "[P12]í¬ë¡¤ë§ì¤€ë¹„_ì‹œêµ¬ë‹¨ì§€ëª…", combined)

    return df
    
def preprocess_13(df): # "[P12]í¬ë¡¤ë§ì¤€ë¹„_ì‹œêµ¬ë‹¨ì§€ëª…"ë¥¼ m.land.naver.comì—ì„œ í¬ë¡¤ë§í•´ì„œ url ì• ì„œ mkerid ê°€ì ¸ì˜¤ê¸°
    '''
    1. step10_csvì˜ "[P12]í¬ë¡¤ë§ì¤€ë¹„_ì‹œêµ¬ë‹¨ì§€ëª…" ì—´ì„ web crawling í• ê±°ì„. 
    '''
    return crawl(df, "[P12]í¬ë¡¤ë§ì¤€ë¹„_ì‹œêµ¬ë‹¨ì§€ëª…", "[P13]markerid")

def preprocess_14(df):
    df
