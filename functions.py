import requests
import pandas as pd
import numpy as np
import json
import webbrowser
import os
import re
import ast
import time
import threading
from tqdm import tqdm
from bs4 import BeautifulSoup
from urllib.parse import quote
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from concurrent.futures import ThreadPoolExecutor, as_completed
from selenium.webdriver.support import expected_conditions as EC


# Set display options for better spacing
pd.set_option('display.max_columns', None)        # show all columns
pd.set_option('display.width', 1000)              # set max line width
pd.set_option('display.max_colwidth', None)       # don't truncate cell values
pd.set_option('display.unicode.east_asian_width', True)  # better for Korean spacing

from config import EXCEL_FILES, COMBINED_EXCEL_OUTPUT, COMBINED_CSV_OUTPUT
from config import TARGET_SIDO_CODES, BASE_GUNGU_URL, BASE_DONG_URL, BASE_APT_URL, BASE_HEADERS, BASE_SIDO_URL

'''[NAVER] ë„¤ì´ë²„ ë¶€ë™ì‚°ì—ì„œ ëª¨ë“  markerid ê°€ì ¸ì˜¤ê¸°'''
def get_sido_info():
    """Fetch list of ì‹œë„ (provinces) with codes and names."""
    r = requests.get(BASE_SIDO_URL, data={"sameAddressGroup": "false"}, headers=BASE_HEADERS)
    r.encoding = "utf-8-sig"
    temp = json.loads(r.text)
    return temp["regionList"] 
def get_gungu_info(sido_code):
    r = requests.get(BASE_GUNGU_URL + sido_code, data={"sameAddressGroup": "false"}, headers=BASE_HEADERS)
    r.encoding = "utf-8-sig"
    temp = json.loads(r.text)
    return temp["regionList"]
def get_dong_info(gungu_code):
    r = requests.get(BASE_DONG_URL + gungu_code, data={"sameAddressGroup": "false"}, headers=BASE_HEADERS)
    r.encoding = "utf-8-sig"
    temp = json.loads(r.text)
    return temp["regionList"]
def get_apt_list(dong_code):
    r = requests.get(BASE_APT_URL + dong_code + '&realEstateType=APT&order=', data={"sameAddressGroup": "false"}, headers=BASE_HEADERS)
    r.encoding = "utf-8-sig"
    temp = json.loads(r.text)
    try:
        return temp['complexList']
    except:
        return []
#ê° ì‹œì— ëŒ€í•œ ê° êµ¬ì— ëŒ€í•œ ê° ë™ì— ëŒ€í•œ ê° ì•„íŒŒíŠ¸ ì½”ë“œë¥¼ ê°€ì ¸ì™€ì„œ ì´í•©í•˜ê¸° df
def make_df():
    sido_list = get_sido_info()
    results = []
    
    for sido in sido_list:
        if sido['cortarNo'] not in TARGET_SIDO_CODES:
            continue

        gungu_list = get_gungu_info(sido['cortarNo'])
        for gungu in tqdm(gungu_list):
            dong_list = get_dong_info(gungu['cortarNo'])
            for dong in dong_list:
                apt_list = get_apt_list(dong['cortarNo'])
                for apt in apt_list:
                    print(
                        apt['complexNo'],     # ì•„íŒŒíŠ¸ primary key
                        sido['cortarName'],   # ì‹œë„ name
                        gungu['cortarName'],  # êµ°êµ¬ name
                        dong['cortarName'],   # ë™ name
                        apt['complexName']    # ì•„íŒŒíŠ¸ name
                    )
                    results.append({
                        'complexNo': apt['complexNo'], # markerid - ê³ ìœ í‚¤
                        'sido': sido['cortarName'],
                        'gungu': gungu['cortarName'],
                        'dong': dong['cortarName'],
                        'complexName': apt['complexName']
                    })
    df = pd.DataFrame(results)
    return df


'''ì—‘ì…€ íŒŒì¼ë“¤ì„ í•˜ë‚˜ì˜ dfë¡œ í•©ì³ì£¼ê¸° -- configì—ì„œ MOLIT ì—‘ì…€ ìë£Œ í•©ì¹˜ê¸° ìœ„í•¨'''    
def combine_excel(file_list):
    """Combine multiple Excel (.xlsx) files into one DataFrame with new index."""
    df_list = []
    for file in file_list:
        try:
            df = pd.read_excel(file)  # Excel doesnâ€™t need encoding
            df_list.append(df)
            print(f"âœ… Loaded: {file} ({len(df)} rows)")
        except Exception as e:
            print(f"âŒ Failed to load {file}: {e}")
    
    if df_list:
        combined_df = pd.concat(df_list, ignore_index=True)
        combined_df.reset_index(drop=True, inplace=True)
        return combined_df
    else:
        print("âš ï¸ No valid Excel files found.")
        return pd.DataFrame()

'''df í¸í•˜ê²Œ ë³´ëŠ” ë°©ë²• í•¨ìˆ˜'''
def preview(df, filename="df_preview.html"):
    style = """
    <style>
        table {
            border-collapse: collapse;
            width: 100%;
        }
        th, td {
            white-space: nowrap;
            padding: 6px 10px;
            border: 1px solid #ccc;
            text-align: center;
        }
        thead {
            background-color: #f2f2f2;
            position: sticky;
            top: 0;
        }
        .scroll-container {
            overflow-x: auto;
            width: 100%;
        }
        body {
            font-family: sans-serif;
        }
    </style>
    """

    html = df.to_html(index=False, escape=False)
    full_html = f"{style}<div class='scroll-container'>{html}</div>"

    with open(filename, "w", encoding="utf-8") as f:
        f.write(full_html)

    webbrowser.open("file://" + os.path.abspath(filename))
    print(f"âœ… Scrollable preview opened: {os.path.abspath(filename)}")

# =================================================================================================
'''Storing and Loading functions'''
def load_step(n):
    path = f"res csv/step_{n}.csv"
    if not os.path.exists(path):
        raise FileNotFoundError(f"step {n} result not found at {path}")
    return pd.read_csv(path) 
def load_csv(file_name):
    path = f"res csv/{file_name}.csv"
    if not os.path.exists(path):
        raise FileNotFoundError(f"{file_name} result not found at {path}")
    return pd.read_csv(path)
def store_result(df, file_name):
    path = f"res csv/{file_name}.csv"
    if not os.path.exists(path):
        df.to_csv(path, index=False)
        print(f"âœ… Saved {file_name} result to {path}")
    else:
        print(f"ğŸ“‚ Step {file_name} already exists: {path}")

#==================================================================================================
'''ê°ì¢… Tools. part of preprocessing'''
def unique_df(df, key_column):
    """
    [ROLE] Keep only rows with unique values in `key_column`.

    Parameters:
    - df: a DataFrame or string name of a CSV in 'res csv/'
    - key_column: column to check uniqueness on

    Returns:
    - DataFrame with only the first occurrence of each unique key_column value
    """
    print(f"{"Original data shape: ", df.shape}")
    df = load_csv(df) if isinstance(df, str) else df
    # Drop duplicates based on key_column, keeping the first occurrence
    df_unique = df.drop_duplicates(subset=key_column, keep="first").reset_index(drop=True)
    print(f"{"Unique data shape: ", df_unique.shape}" )
    
    
    return df_unique
    
def mapping(source_file, source_column, target_file, target_column, call_column, insert_at, new_col_name):
    '''[ROLE] source_fileì˜ source_column ê°’ì„ target_fileì˜ target_columnì—ì„œ ì°¾ì•„ë³´ê³  call_columnê°€ì ¸ì˜¤ê¸°
        Parameters:
    - source_file: str(csv) or df
    - source_column: column to match from source_file
    - target_file: str(csv) or df
    - target_column: column to match from target_file
    - call_column: column to retrieve from target_file
    - insert_at: 'first', 'last', or (int) index to insert the new column after (int)th column
    - new_col_name: name for the new column in source_df'''
    

    # Load if CSV, else assume already DataFrame
    source_df = load_csv(source_file) if isinstance(source_file, str) else source_file
    target_df = load_csv(target_file) if isinstance(target_file, str) else target_file
    
    # Build mapping dictionary
    mapping_dict = dict(zip(target_df[target_column], target_df[call_column]))
    # Perform mapping
    mapped_series = source_df[source_column].map(mapping_dict)
    
    # Log mapping success and fill unmapped
    total = len(mapped_series)
    mapped = mapped_series.notna().sum()
    unmapped = total - mapped
    print(f"âœ… {mapped}/{total} values successfully mapped ({unmapped} unmapped).")

    mapped_series = mapped_series.fillna("UNMAPPED")
    
    # Determine insert index
    if insert_at == "first":
        insert_idx = 0
    elif insert_at == "last":
        insert_idx = len(source_df.columns)
    elif isinstance(insert_at, int):
        insert_idx = min(insert_at + 1, len(source_df.columns))
    else:
        raise ValueError("insert_at must be 'first', 'last', or an integer")

    # Insert new column
    source_df.insert(insert_idx, new_col_name, mapped_series)

    return source_df

def update_key(df):
    """
    [ROLE] Create or update [KEY]markerid column at index 0.
    Ensures most recent [P#]markerid columns are ordered by descending step (P10, P7, P5, etc.).
    """
    # 1. Find all [P#]markerid columns (e.g. [P10]markerid)
    candidate_cols = [col for col in df.columns if re.match(r"\[P\d+\]markerid", col)]

    if not candidate_cols:
        raise ValueError("âŒ No valid [P#]markerid columns found in DataFrame.")

    # 2. Sort them in descending P# order (most recent first)
    candidate_cols_sorted = sorted(
        candidate_cols, key=lambda c: int(re.findall(r"\[P(\d+)\]", c)[0]), reverse=True
    )

    # 3. Generate unified markerid
    final_markerid = df[candidate_cols_sorted].apply(
        lambda row: next((val for val in row if val != "UNMAPPED"), "UNMAPPED"), axis=1
    )

    # 4. Remove existing [KEY]markerid if present
    if "[KEY]markerid" in df.columns:
        df.drop(columns=["[KEY]markerid"], inplace=True)

    # 5. Build final column order
    remaining_cols = [col for col in df.columns if col not in candidate_cols_sorted]
    new_col_order = candidate_cols_sorted + remaining_cols  # no [KEY] yet

    df = df[new_col_order]  # reorder
    df.insert(0, "[KEY]markerid", final_markerid)  # insert at front

    # 6. Log
    total = len(final_markerid)
    success = (final_markerid != "UNMAPPED").sum()
    print(f"âœ… [KEY]markerid: {success}/{total} mapped successfully.")
    print(f"   Used columns (ordered): {candidate_cols_sorted}")

    return df
    
def col_type(df, col_name, target_type):
    """
    [ROLE] Convert a specific column in a df to a given type.
    Supported types: 'int', 'float', 'str'

    Example:
        df = col_type(df, '[P10]markerid', 'int')
    """
    if col_name not in df.columns:
        raise ValueError(f"âŒ Column '{col_name}' not found in DataFrame.")

    if target_type == "int":
        df[col_name] = df[col_name].apply(
            lambda x: int(float(x)) if pd.notna(x) and str(x).replace('.', '', 1).isdigit() else x
        )
    elif target_type == "float":
        df[col_name] = df[col_name].apply(
            lambda x: float(x) if pd.notna(x) and str(x).replace('.', '', 1).replace('-', '', 1).replace(',', '', 1).replace(' ', '') != "" else x
        )
    elif target_type == "str":
        df[col_name] = df[col_name].astype(str)
    else:
        raise ValueError(f"âŒ Unsupported target type: {target_type}")

    print(f"ğŸ”§ column '{col_name}' to type '{target_type}' âœ…")
    return df

def update_key(df): # [MOLIT] step_5.csvì— 1ì—´ All_markerid ì—´ ì¶”ê°€í•˜ê¸°
    """
    [ROLE] Create or update [KEY]markerid column at index 0.
    Ensures most recent [P#]markerid columns are ordered by descending step (P10, P7, P5, etc.).
    """
    # 1. Find all [P#]markerid columns (e.g. [P10]markerid)
    candidate_cols = [col for col in df.columns if re.match(r"\[P\d+\]markerid", col)]

    if not candidate_cols:
        raise ValueError("âŒ No valid [P#]markerid columns found in DataFrame.")

    # 2. Sort them in descending P# order (most recent first)
    candidate_cols_sorted = sorted(
        candidate_cols, key=lambda c: int(re.findall(r"\[P(\d+)\]", c)[0]), reverse=True
    )

    # 3. Generate unified markerid
    final_markerid = df[candidate_cols_sorted].apply(
        lambda row: next((val for val in row if val != "UNMAPPED"), "UNMAPPED"), axis=1
    )

    # 4. Remove existing [KEY]markerid if present
    if "[KEY]markerid" in df.columns:
        df.drop(columns=["[KEY]markerid"], inplace=True)

    # 5. Build final column order
    remaining_cols = [col for col in df.columns if col not in candidate_cols_sorted]
    new_col_order = candidate_cols_sorted + remaining_cols  # no [KEY] yet

    df = df[new_col_order]  # reorder
    df.insert(0, "[KEY]markerid", final_markerid)  # insert at front

    # 6. Log
    total = len(final_markerid)
    success = (final_markerid != "UNMAPPED").sum()
    print(f"âœ… [KEY]markerid: {success}/{total} mapped successfully.")
    print(f"   Used columns (ordered): {candidate_cols_sorted}")

    return df

def count_unmapped(df):
    unmapped_count = (df.iloc[:, 0] == "UNMAPPED").sum()
    print(f"ğŸ” UNMAPPED count: {unmapped_count}")

def update_key_new(df):
    df = df.copy()
    second_col = df.columns[1]

    condition = (
        df[second_col].apply(lambda x: str(x).isdigit()) &
        (df["[KEY]markerid"] == "UNMAPPED")
    )

    update_count = condition.sum()
    df.loc[condition, "[KEY]markerid"] = df.loc[condition, second_col]

    print(f"âœ… Updated {update_count} rows from {second_col} to [KEY]markerid.")
    return df


# Thread-local storage to keep one driver per thread
thread_local = threading.local()

def get_driver():
    if not hasattr(thread_local, "driver"):
        options = Options()
        options.add_argument("--headless")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--log-level=3")
        thread_local.driver = webdriver.Chrome(options=options)
    return thread_local.driver

def crawl(df, source_col, new_col_name, max_workers=5):
    target_rows = df[df["[KEY]markerid"] == "UNMAPPED"].copy()
    print(f"ğŸ” Crawling markerIds for {len(target_rows)} unmapped entries...")

    results = {}

    def process_row(index, search_term):
        driver = get_driver()
        result = "UNMAPPED"
        try:
            driver.get("https://m.land.naver.com/search")
            WebDriverWait(driver, 4).until(EC.presence_of_element_located((By.ID, "query")))
            search_box = driver.find_element(By.ID, "query")
            search_box.clear()
            search_box.send_keys(search_term)
            search_box.send_keys(Keys.ENTER)
            time.sleep(2.5)

            url = driver.current_url
            match = re.search(r"/complexes/(\d+)", url)
            if match:
                result = match.group(1)
                print(f"âœ… {search_term} â†’ {result}")
            else:
                print(f"âŒ {search_term} â†’ MarkerID not found")
        except Exception as e:
            print(f"ğŸš¨ Error for '{search_term}': {e}")
        return index, result

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(process_row, idx, row[source_col]): idx
            for idx, row in target_rows.iterrows()
        }

        for future in tqdm(as_completed(futures), total=len(futures), desc="ğŸ”„ Crawling"):
            idx, markerid = future.result()
            results[idx] = markerid

    # Update results into DataFrame
    df[new_col_name] = "UNMAPPED"
    for idx, markerid in results.items():
        df.at[idx, new_col_name] = markerid

    # Move the new column next to [KEY]markerid
    if new_col_name in df.columns:
        df = df.drop(columns=[new_col_name])
    insert_idx = df.columns.get_loc("[KEY]markerid") + 1
    df.insert(insert_idx, new_col_name, [results.get(i, "UNMAPPED") for i in df.index])

    print(f"ğŸŸ¢ Finished crawling. {sum(v != 'UNMAPPED' for v in results.values())} / {len(results)} mapped.")
    return df

def crawl_id(df,source_column_name, insert_data_after_which_col):
    source = df[source_column_name].tolist()
    
    results = []

    for marker_id in tqdm(source):
        url = f'https://fin.land.naver.com/complexes/{marker_id}?tab=complex-info'
        headers = {
            'User-Agent': 'Mozilla/5.0',
            'Referer': 'https://fin.land.naver.com/',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        }

        try:
            res = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(res.text, 'html.parser')

            # Find the script tag containing the JSON data
            script_tag = next((s.text for s in soup.find_all("script") if '"dehydratedState"' in s.text), None)
            if not script_tag:
                raise ValueError("JSON data not found in page")

            # Extract and parse the JSON object
            json_start = script_tag.find('{"props":')
            json_end = script_tag.rfind('}') + 1
            json_blob = script_tag[json_start:json_end]
            parsed = json.loads(json_blob)
            queries = parsed['props']['pageProps']['dehydratedState']['queries']

            # Extract apartment details from JSON
            data = {}
            for q in queries:
                result = q.get("state", {}).get("data", {}).get("result", {})
                if "address" in result:
                    address = result["address"].get("roadName")
                    zip_code = result["address"].get("zipCode")
                    approval_date = result.get("useApprovalDate")
                    household_count = result.get("totalHouseholdNumber")
                    heating_type = result.get("heatingAndCoolingInfo", {}).get("heatingEnergyType")
                    parking = result.get("parkingInfo", {}).get("totalParkingCount")

                    data = {
                        "[P15]markerId": marker_id,
                        "[P15]ì£¼ì†Œ": address,
                        "[P15]ì‚¬ìš©ìŠ¹ì¸ì¼": approval_date,
                        "[P15]ì„¸ëŒ€ìˆ˜": household_count,
                        "[P15]ë‚œë°©": heating_type,
                        "[P15]ì£¼ì°¨": parking,
                        "[P15]ìš°í¸ë²ˆí˜¸": zip_code
                    }
                    break

            if not data:
                data = {"[P15]markerId": marker_id, "[P15]ì£¼ì†Œ": None, "[P15]ì‚¬ìš©ìŠ¹ì¸ì¼": None, "[P15]ì„¸ëŒ€ìˆ˜": None, "[P15]ë‚œë°©": None, "[P15]ì£¼ì°¨": None, "[P15]ìš°í¸ë²ˆí˜¸": None}

            results.append(data)
            print(data)

        except Exception as e:
            results.append({
                "[P15]markerId": marker_id,
                "[P15]ì£¼ì†Œ": None,
                "[P15]ì‚¬ìš©ìŠ¹ì¸ì¼": None,
                "[P15]ì„¸ëŒ€ìˆ˜": None,
                "[P15]ë‚œë°©": None,
                "[P15]ì£¼ì°¨": None,
                "[P15]ìš°í¸ë²ˆí˜¸": None,
                "[P15]Error": str(e)
            })

    # Convert to DataFrame and show
    results_df = pd.DataFrame(results)
    print(f"[P15] results: first 20 lines")
    print(results_df[:20])
    
    # Reset index to ensure proper merge
    results_df.index = df.index  # ensures alignment with original df

    # Determine insertion point
    insert_index = df.columns.get_loc(insert_data_after_which_col) + 1

    # Split df into parts and insert
    df_front = df.iloc[:, :insert_index]
    df_back = df.iloc[:, insert_index:]
    df_combined = pd.concat([df_front, results_df, df_back], axis=1)

    return df_combined

# NORESULT / MULTIPLE ê²°ê³¼ ë¶„ë¥˜í•´ì¤Œ.
def classify_search_result(url):
    response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
    soup = BeautifulSoup(response.text, 'html.parser')

    if "ê²€ìƒ‰ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤" in soup.text:
        return "NORESULT"
    return "MULTIPLE"

def multiple_id_search(df, col_name):
    # Ensure new column exists and is initially blank
    df["[P14]multiple_results"] = None

    # Insert the new column just to the right of [KEY]markerid
    cols = df.columns.tolist()
    marker_idx = cols.index("[KEY]markerid")
    # Move [P14]multiple_results right after it
    cols.remove("[P14]multiple_results")
    cols.insert(marker_idx + 1, "[P14]multiple_results")
    df = df[cols]  # Reorder columns

    # Target only unmapped rows
    target_rows = df[df["[KEY]markerid"] == "UNMAPPED"].copy()
    print(f"ğŸ” Crawling markerIds for {len(target_rows)} unmapped entries...")

    for idx, row in tqdm(target_rows.iterrows(), total=len(target_rows)):
        term = row[col_name]
        print(f"\nğŸ“Œ Searching: {term}")
        encoded_term = quote(term)
        url = f"https://m.land.naver.com/search/result/{encoded_term}"
        print(f"ğŸ”— URL: {url}")

        result = classify_search_result(url)
        print(f"ğŸ§¾ Result: {result}")

        if result == "NORESULT":
            df.at[idx, "[P14]multiple_results"] = "NORESULT"
        else:
            # MULTIPLE
            response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
            soup = BeautifulSoup(response.text, 'html.parser')
            a_tags = soup.find_all('a', href=True)

            marker_ids = []

            for a in a_tags:
                href = a['href']
                if href.startswith("/complex/info/"):
                    marker_id = href.split("/")[3]
                    marker_ids.append(marker_id)

            print(f"ğŸ“Œ Marker IDs: {marker_ids}")
            df.at[idx, "[P14]multiple_results"] = marker_ids  # stored as list

    return df  # return updated DataFrame

def match_marker_ids_by_region(df_step12, df_markerid):
    """
    Matches candidate markerIds in [P14]multiple_results to actual rows in markerid_3
    based on **ì‹œ, **êµ¬, **ë™.
    Inserts new column [P16]match after [KEY]markerid.
    """

    match_results = []

    for idx, row in df_step12.iterrows():
        raw_candidates = row.get("[P14]multiple_results", "")
        # Skip cases: empty, NORESULT, or "[]"
        if pd.isna(raw_candidates) or raw_candidates.strip() in ["", "NORESULT", "[]"]:
            match_results.append(None)
            continue
        try:
            # Safely parse stringified list (e.g., "['1036', '12345']")
            candidate_ids = ast.literal_eval(raw_candidates)
            if not isinstance(candidate_ids, list):
                match_results.append("NOTFOUND")
                continue
        except Exception:
            match_results.append("NOTFOUND")
            continue

        # Row-level ì‹œ/êµ¬/ë™ to match against
        target_sido = str(row.get("**ì‹œ", "")).strip()
        target_gungu = str(row.get("**êµ¬", "")).strip()
        target_dong = str(row.get("**ë™", "")).strip()
        print(f"source: {target_sido}, {target_gungu}, {target_dong}")

        matched_ids = []
        for marker_id in tqdm(candidate_ids):
            df_markerid["complexNo"] = df_markerid["complexNo"].astype(str)
            match_row = df_markerid[df_markerid["complexNo"] == marker_id]
            if not match_row.empty:
                sido = str(match_row.iloc[0]["sido"]).strip()
                gungu = str(match_row.iloc[0]["gungu"]).strip()
                dong = str(match_row.iloc[0]["dong"]).strip()

                if (sido == target_sido) and (gungu == target_gungu) and (dong == target_dong):
                    matched_ids.append(marker_id)

        # Result logic
        if not matched_ids:
            match_results.append("NOTFOUND")
        elif len(matched_ids) == 1:
            match_results.append(matched_ids[0])
        else:
            match_results.append(matched_ids)

    # Insert result column into original df_step12
    result_series = pd.Series(match_results, name="[P16]match")
    insert_index = df_step12.columns.get_loc("[KEY]markerid") + 1

    df_with_match = pd.concat([
        df_step12.iloc[:, :insert_index],
        result_series,
        df_step12.iloc[:, insert_index:]
    ], axis=1)

    return df_with_match

def check_address_uniqueness(df_step14, df_markerid):
    """
    Checks whether ë„ë¡œëª… in df_step14 and [P15]ì£¼ì†Œ in df_markerid are unique.
    If not, prints the duplicated values with counts.
    """

    print("ğŸ” Checking uniqueness of ë„ë¡œëª… in step_14...")
    if df_step14["ë„ë¡œëª…"].is_unique:
        print("âœ… 'ë„ë¡œëª…' is unique in step_14.")
    else:
        print("âŒ 'ë„ë¡œëª…' is NOT unique. Duplicates:")
        dup_road = df_step14["ë„ë¡œëª…"].value_counts()
        print(dup_road[dup_road > 1].head(10))  # show top 10 duplicates

    print("\nğŸ” Checking uniqueness of [P15]ì£¼ì†Œ in markerid_3...")
    if df_markerid["[P15]ì£¼ì†Œ"].is_unique:
        print("âœ… '[P15]ì£¼ì†Œ' is unique in markerid_3.")
    else:
        print("âŒ '[P15]ì£¼ì†Œ' is NOT unique. Duplicates:")
        dup_addr = df_markerid["[P15]ì£¼ì†Œ"].value_counts()
        print(dup_addr[dup_addr > 1].head(10))  # show top 10 duplicates


# =================================================================================================
'''[MOLIT] Preprocessing functions'''
# ê° ë‹¨ê³„ì˜ ê²°ê³¼ëŠ” res fileì— ì €ì¥ë¨. ë‹¤ìŒ ë‹¨ê³„ì—ì„œëŠ” ì´ì „ ë‹¨ê³„ ê²°ê³¼ íŒŒì¼ ë¶ˆëŸ¬ì™€ì„œ ì‘ë™.
"""Load the result of step n from res csv/step_n.csv"""
# ì…ë ¥: step n # ì¶œë ¥: step_n.csvë¥¼ dfë¡œ return

def preprocess_1(df): # [MOLIT] ê´‘ì—­ì‹œ --> ì‹œ
    '''[ROLE] **ê´‘ì—­ì‹œ --> **ì‹œ ë¡œ ë°”ê¾¸ëŠ” ì‘ì—… & **ì‹œ **êµ¬ **ë™ --> ê°ì—´ë¡œ ì°¢ê¸°'''
    
    
    '''1. **ê´‘ì—­ì‹œ --> **ì‹œ ë¡œ ë°”ê¾¸ëŠ” ì‘ì—…'''
    #res csv/step_0.csv --> 2ì—´(ì‹œêµ°êµ¬)
    city_replacements = {
        "ê´‘ì£¼ê´‘ì—­ì‹œ": "ê´‘ì£¼ì‹œ",
        "ëŒ€ì „ê´‘ì—­ì‹œ": "ëŒ€ì „ì‹œ",
        "ëŒ€êµ¬ê´‘ì—­ì‹œ": "ëŒ€êµ¬ì‹œ",
        "ë¶€ì‚°ê´‘ì—­ì‹œ": "ë¶€ì‚°ì‹œ",
    }
    def replace_city_name(value):
        for old, new in city_replacements.items():
            if value.startswith(old):
                return value.replace(old, new, 1)
        return value

    df["ì‹œêµ°êµ¬"] = df["ì‹œêµ°êµ¬"].apply(replace_city_name)
    
  
    '''2. **ì‹œ **êµ¬ **ë™ --> ê°ì—´ë¡œ ì°¢ê¸°'''
    if "ì‹œêµ°êµ¬" not in df.columns:
        raise ValueError("âŒ 'ì‹œêµ°êµ¬' column not found in the DataFrame.")
    # Find index of ì‹œêµ°êµ¬ column
    
    # Split 'ì‹œêµ°êµ¬' into 3 parts: col names: **ì‹œ, **êµ¬, **ë™
    split_cols = df["ì‹œêµ°êµ¬"].str.split(" ", n=2, expand=True)
    split_cols.columns = ["**ì‹œ", "**êµ¬", "**ë™"]
    insert_idx = df.columns.get_loc("ì‹œêµ°êµ¬") + 1
    
    for i, col_name in enumerate(split_cols.columns):
        df.insert(insert_idx + i, col_name, split_cols[col_name])
    
    return df

def preprocess_2(df): # [MOLIT] ì‹œêµ°êµ¬ë‹¨ì§€ëª… col ë§Œë“¤ê¸°
    '''[ROLE] make ì‹œêµ°êµ¬ + ë‹¨ì§€ëª… into 1 column. Clean '**ë™' if needed, then insert after '**ë™'.'''

    if "**ë™" not in df.columns:
        raise ValueError("âŒ '**ë™' column not found in DataFrame.")
    
    # Step 1: Clean '**ë™' values (keep only the first word if there's a space)
    df["**ë™"] = df["**ë™"].astype(str).str.split().str[0]

    # Step 2: Combine ì‹œ + êµ¬ + cleaned ë™ + ë‹¨ì§€ëª…
    combined = (
        df["**ì‹œ"].astype(str) + " " +
        df["**êµ¬"].astype(str) + " " +
        df["**ë™"].astype(str) + " " +
        df["ë‹¨ì§€ëª…"].astype(str)
    )

    # Step 3: Insert new column after '**ë™'
    insert_idx = df.columns.get_loc("**ë™") + 1
    df.insert(insert_idx, "[P2]ì‹œêµ°êµ¬_ë‹¨ì§€ëª…", combined)

    return df

def preprocess_3(df): #[MOLIT] Unique
    '''[ROLE] UNIQUEí•œ ì‹œêµ°êµ¬ ë‹¨ì§€ëª…ë§Œ ë‚¨ê¸°ê¸° from MOLIT = step_2'''
    df3 = unique_df(df, "[P2]ì‹œêµ°êµ¬_ë‹¨ì§€ëª…")
    return df3

def preprocess_4(df): # [NAVER markerid] ì‹œêµ°êµ¬ë‹¨ì§€ëª… col ë§Œë“¤ê¸°
    '''[ROLE] make ì‹œêµ°êµ¬ ë‹¨ì§€ëª… into 1 column. insert after complexName column'''    
    # Step 1: Create new column
    combined = df["sido"].astype(str) + " " + df["gungu"].astype(str) + " " + df["dong"].astype(str) +" " + df["complexName"].astype(str)
    if "complexName" not in df.columns:
        raise ValueError("âŒ 'complexName' column not found in DataFrame.")
    
    # Step 2: Insert index
    insert_idx = df.columns.get_loc("complexName") + 1  # insert AFTER 'complexName'
    # Step 3: Insert new column
    df.insert(insert_idx, "[P4]ì‹œêµ°êµ¬_ë‹¨ì§€ëª…", combined)

    return df
    
def preprocess_5(df): # [mapping] 1
    return mapping(
    source_file=df,
    source_column="[P2]ì‹œêµ°êµ¬_ë‹¨ì§€ëª…",
    target_file="markerid_1",
    target_column="[P4]ì‹œêµ°êµ¬_ë‹¨ì§€ëª…",
    call_column="complexNo",
    insert_at="first",
    new_col_name="[P5]markerid"
    )   
    
def preprocess_6(df): # [NAVER markerid] "(ì£¼ìƒë³µí•©)", "(ë„ì‹œí˜•)" ì§€ìš°ê¸°
    '''[ROLE] df(markerid)remove "(ì£¼ìƒë³µí•©)", "(ë„ì‹œí˜•)" from complexName'''
    
    col_to_clean = "[P4]ì‹œêµ°êµ¬_ë‹¨ì§€ëª…"
    new_col_name = "[P6]ì‹œêµ°êµ¬_ë‹¨ì§€ëª…_cleaned_(ì£¼ìƒë³µí•©)(ë„ì‹œí˜•)"

    if col_to_clean not in df.columns:
        raise ValueError(f"âŒ '{col_to_clean}' column not found in DataFrame.")

    # Step 1: Clean the target column
    cleaned = (
        df[col_to_clean]
        .astype(str)
        .str.replace("(ì£¼ìƒë³µí•©)", "", regex=False)
        .str.replace("(ë„ì‹œí˜•)", "", regex=False)
        .str.strip()
    )

    # Step 2: Insert cleaned version right after the original column
    insert_idx = df.columns.get_loc(col_to_clean) + 1
    df.insert(insert_idx, new_col_name, cleaned)

    return df
    
def preprocess_7(df): # [mapping] 2
    return mapping(
    source_file=df,
    source_column="[P2]ì‹œêµ°êµ¬_ë‹¨ì§€ëª…",
    target_file="markerid_2",
    target_column="[P6]ì‹œêµ°êµ¬_ë‹¨ì§€ëª…_cleaned_(ì£¼ìƒë³µí•©)(ë„ì‹œí˜•)",
    call_column="complexNo",
    insert_at="first",
    new_col_name="[P7]markerid"
    )

def preprocess_8(df): # [MOLIT] 1ì—´ All_markerid ì—´ ì¶”ê°€í•˜ê¸°
    return update_key(df)

def preprocess_9(df): # [MOLIT] ë‹¨ì§€ëª…ì—ì„œ '**ë™' prefix ì§€ìš°ê¸°
    '''[ROLE] If ë‹¨ì§€ëª… starts with '**ë™', remove that prefix and insert cleaned version after [P2]ì‹œêµ°êµ¬_ë‹¨ì§€ëª….'''
    if "**ë™" not in df.columns or "ë‹¨ì§€ëª…" not in df.columns or "[P2]ì‹œêµ°êµ¬_ë‹¨ì§€ëª…" not in df.columns:
        raise ValueError("âŒ Missing one or more required columns: '**ë™', 'ë‹¨ì§€ëª…', '[P2]ì‹œêµ°êµ¬_ë‹¨ì§€ëª…'.")

    # Step 1: Remove prefix from ë‹¨ì§€ëª… if it matches **ë™
    cleaned = df.apply(
        lambda row: row["ë‹¨ì§€ëª…"][len(row["**ë™"]):] if str(row["ë‹¨ì§€ëª…"]).startswith(str(row["**ë™"])) else row["ë‹¨ì§€ëª…"],
        axis=1
    )

    # Step 2: Insert new column after [P2]ì‹œêµ°êµ¬_ë‹¨ì§€ëª…
    insert_idx = df.columns.get_loc("[P2]ì‹œêµ°êµ¬_ë‹¨ì§€ëª…") + 1
    df.insert(insert_idx, "[P9]ë‹¨ì§€ëª…_erased_**ë™", cleaned)

    return df

def preprocess_10(df): # [mapping] 3
    '''[ROLE] step_6ì˜ 'ë‹¨ì§€ëª…' prefixì—ì„œ '**ë™' ì§€ìš´ê±° mapping í•˜ê¸°.'''
    return mapping(
    source_file=df,
    source_column="[P9]ë‹¨ì§€ëª…_erased_**ë™",
    target_file="markerid_2",
    target_column="[P6]ì‹œêµ°êµ¬_ë‹¨ì§€ëª…_cleaned_(ì£¼ìƒë³µí•©)(ë„ì‹œí˜•)",
    call_column="complexNo",
    insert_at=2,
    new_col_name="[P10]markerid"
    )

def preprocess_11(df): # [MOLIT] markerid ì—´ë“¤ì„ ì†Œìˆ˜ì ì—ì„œ int ë¡œ ë°”ê¿”ì£¼ê¸°
    df = col_type(df, "[KEY]markerid", "int")
    df = col_type(df, "[P10]markerid", "int")
    df = col_type(df, "[P7]markerid", "int")
    df = col_type(df, "[P5]markerid", "int")
    return df

def preprocess_12(df): # [MOLIT] "**ì‹œ + **êµ¬ + '[P9]ë‹¨ì§€ëª…_erased_**ë™' " ì—´ ë§Œë“¤ê¸°. ê²€ìƒ‰ì¤€ë¹„ì‹œì¼œì£¼ê¸°
    '''ì›¹í¬ë¡¤ë§ ì¤€ë¹„ì‹œì¼œì£¼ê¸° ì‘ì—…. ê´‘ì£¼ì‹œ ë¶êµ¬ ìš´ì•”ë™ ìš´ì•”1ì°¨ë‚¨ì–‘íœ´íŠ¼ --> ê´‘ì£¼ ë¶êµ¬ ìš´ì•”1ì°¨ë‚¨ì–‘íœ´íŠ¼
        "**ë™" ì§€ìš°ê¸° (ë™ì„ ì§€ìš°ê³  ë„¤ì´ë²„ì— ê²€ìƒ‰í•´ì•¼ì§€ í¬ë¡¤ë§ì— ì˜ ë‚˜ì˜´.)
    '''
    # Combine ì‹œ + êµ¬ + ë‹¨ì§€ëª…
    combined = (
        df["**ì‹œ"].astype(str) + " " +
        df["**êµ¬"].astype(str) + " " +
        df["[P9]ë‹¨ì§€ëª…_erased_**ë™"].astype(str)
    )

    # Insert new column after '**ë™'
    insert_idx = df.columns.get_loc("[P9]ë‹¨ì§€ëª…_erased_**ë™") + 1
    df.insert(insert_idx, "[P12]í¬ë¡¤ë§ì¤€ë¹„_ì‹œêµ¬ë‹¨ì§€ëª…", combined)

    return df
    
def preprocess_13(df): # "[P12]í¬ë¡¤ë§ì¤€ë¹„_ì‹œêµ¬ë‹¨ì§€ëª…"ë¥¼ m.land.naver.comì—ì„œ í¬ë¡¤ë§í•´ì„œ url ì• ì„œ mkerid ê°€ì ¸ì˜¤ê¸°
    '''
    1. step10_csvì˜ "[P12]í¬ë¡¤ë§ì¤€ë¹„_ì‹œêµ¬ë‹¨ì§€ëª…" ì—´ì„ web crawling í• ê±°ì„. 
    '''
    return crawl(df, "[P12]í¬ë¡¤ë§ì¤€ë¹„_ì‹œêµ¬ë‹¨ì§€ëª…", "[P13]markerid")

def preprocess_14(df): # "[P12]í¬ë¡¤ë§ì¤€ë¹„_ì‹œêµ°êµ¬ë‹¨ì§€ëª…"ì„ ê²€ìƒ‰í–ˆì„ë•Œ ì—¬ëŸ¬ê°’ ë‚˜ì˜¤ëŠ” markeridë“¤ ë‹¤ ë¶ˆëŸ¬ì™€ì„œ ê¸°ë¡í•˜ê¸°.
    return multiple_id_search(df, "[P12]í¬ë¡¤ë§ì¤€ë¹„_ì‹œêµ¬ë‹¨ì§€ëª…")
    
def preprocess_15(df): # [markerid_3]ì˜ complexNoë¥¼ ë„¤ì´ë²„ í¬ë¡¤ë§í•´ì„œ "[P6]..."ì—´ ë’¤ì— ì •ë³´ ì‚½ì…í•˜ê¸° (ex: [P15]ì£¼ì†Œ, [P15]ì£¼ì°¨) 
    return crawl_id(df, "complexNo", "[P6]ì‹œêµ°êµ¬_ë‹¨ì§€ëª…_cleaned_(ì£¼ìƒë³µí•©)(ë„ì‹œí˜•)")

def preprocess_16(df): # [MOLIT]ì˜ [P14]multiple_resultsì—´ì˜ ê°’ë“¤ì— ëŒ€í•œ **ì‹œ, **êµ¬, **ë™ì„ markerid_3ì˜ sido, gungu, dongì´ë‘ ë¹„êµ
    markerid_3_df = load_csv('markerid_3')
    return match_marker_ids_by_region(df,markerid_3_df)

def preprocess_17(df): # [MOLIT]ì˜ [P16]matchì—´ì´ í•˜ë‚˜ì˜ ê°’ë§Œ ìˆìœ¼ë©´ update [KEY]markeridí•´ì¤Œ.
    """
    if dfì˜ column "[P16]match" has only one value 
        then: ê·¸ valueë§Œ "[KEY]markerid" ì—´ì— ì—…ë°ì´íŠ¸ í•˜ê¸°.
    For rows where [P16]match contains a single ID (not a list, not empty, not NOTFOUND),
    update [KEY]markerid with that ID. Otherwise, leave as is.
    Prints how many rows were updated and how many are still UNMAPPED.
    """
    updated_markerids = []
    updated_count = 0

    for idx, row in df.iterrows():
        match_val = row.get("[P16]match")

        if pd.isna(match_val) or match_val in ["", "NOTFOUND"]:
            updated_markerids.append(row["[KEY]markerid"])
            continue

        if isinstance(match_val, str) and match_val.startswith("[") and match_val.endswith("]"):
            updated_markerids.append(row["[KEY]markerid"])
            continue
        
        updated_markerids.append(match_val)
        updated_count += 1

    # Apply updated markerids
    df = df.copy()
    df["[KEY]markerid"] = updated_markerids

    # Count how many are still unmapped
    still_unmapped = (df["[KEY]markerid"] == "UNMAPPED").sum()
    total_rows = len(df)
    mapped = total_rows-still_unmapped
    print(f"âœ…[P17] Updated {updated_count} rows")
    print(f"ğŸ” {mapped}/{total_rows} Done. Still unmapped: {still_unmapped} rows")

    return df

def preprocess_18(df_step14, df_markerid): # uniqueë„ë¡œëª… & unique[P15]ì£¼ì†Œ mapí•´ì„œ complexNo ê°€ì ¸ì˜´
    """
    Create a new column [P18]markerid in df_step14 by matching ë„ë¡œëª… (from df_step14)
    with [P15]ì£¼ì†Œ (from df_markerid), only if both sides are unique.
    Returns the updated df_step14 with the new column inserted after [KEY]markerid.
    """
    def update_markerid_from_P18(df):
        df = df.copy()
        total_rows = len(df)
        
        condition = (
            (df["[KEY]markerid"] == "UNMAPPED") &
            (df["[P18]markerid"].notna()) &
            (df["[P18]markerid"] != "DUPLICATE")
        )

        df.loc[condition, "[KEY]markerid"] = df.loc[condition, "[P18]markerid"]
        
        total_mapped = (df["[KEY]markerid"] != "UNMAPPED").sum()
        
        print(f"âœ… [KEY]markerid updated from [P18]markerid: {condition.sum()} rows")
        print(f"ğŸ“Š Total mapped: {total_mapped} / {total_rows}")
        
        return df

    df = df_step14.copy()

    # Step 1: Identify non-unique ë„ë¡œëª… in df_step14
    duplicated_roadnames = set(df_step14["ë„ë¡œëª…"][df_step14["ë„ë¡œëª…"].duplicated(keep=False)])

    # Step 2: Identify non-unique [P15]ì£¼ì†Œ in df_markerid
    duplicated_addresses = df_markerid["[P15]ì£¼ì†Œ"][df_markerid["[P15]ì£¼ì†Œ"].duplicated(keep=False)]

    # Step 3: Create a mapping from ì£¼ì†Œ to complexNo (only for unique addresses)
    unique_markerid = df_markerid[~df_markerid["[P15]ì£¼ì†Œ"].isin(duplicated_addresses)]
    address_to_id = dict(zip(unique_markerid["[P15]ì£¼ì†Œ"], unique_markerid["complexNo"]))

    # Step 4: Initialize the new column
    new_col = []

    unique_processed = 0
    duplicates_skipped = 0
    no_match = 0

    for _, row in df.iterrows():
        roadname = row["ë„ë¡œëª…"]

        if roadname in duplicated_roadnames:
            new_col.append("DUPLICATE")
            duplicates_skipped += 1
        elif roadname in address_to_id:
            new_col.append(address_to_id[roadname])
            unique_processed += 1
        else:
            new_col.append(None)
            no_match += 1

    # Step 5: Insert the new column after [KEY]markerid
    insert_index = df.columns.get_loc("[KEY]markerid") + 1
    df.insert(insert_index, "[P18]markerid", new_col)

    # Step 6: Print summary
    total = len(df)
    print(f"âœ… [P18]Unique rows processed and matched: {unique_processed}")
    print(f"âŒ [P18]Duplicates skipped: {duplicates_skipped}")
    print(f"ğŸ” [P18]Rows with no match found: {no_match}")
    print(f"ğŸ“Š [P18]Total rows: {total}")
    
    df = update_markerid_from_P18(df)
    return df

    
    
    
    
    
    # 0. ìƒˆë¡œìš´ ì—´ ë§Œë“¤ê¸° = [P18]markerid: step_14 ì—ëŒ€ê°€ [KEY]markerid ë‹¤ìŒì— ì¶”ê°€í• ê±°ì„. 
    # 1. not unique í•œ ì• ë“¤ ì°¾ê³  ì œì™¸í•˜ê¸°. ìƒˆë¡œìš´ ì—´ì— DUPLICATEì´ë¼ê³  ê¸°ì…
    # 2. uniqueí•œ ì• ë“¤ë¡œë§Œ ê°€ì§€ê³  ë†€ê±°ì„.
    # 3. row by row ë‚´ë ¤ê°€ë©´ì„œ: if ë„ë¡œëª… is unique, then search in markerid_3ì˜ [P15]ì£¼ì†Œì— match í•´ì„œ ê²°ê³¼ë¡œ complexNoê°€ì ¸ì™€ì„œ ìƒˆë¡œìš´ ì—´ê²Œ ê¸°ì…. 
    
def preprocess_19(df_edge_case, df_markerid3): # [EDGE_CASE]ì¤‘ì—ì„œ unique"ë„ë¡œëª…"& unique markerid_3  
    """
    df_edge_caseì˜ "ë„ë¡œëª…" column ì—ì„œ Duplicate ê°’ë“¤ ì œì™¸. Unique valueë“¤ì— ëŒ€í•´ì„œë§Œ ê°ì df_markerid3ì— ê²€ìƒ‰í•´ì„œ, complexNo ê°€ì ¸ì˜´. 
        df_markerid3ì˜ "[P15]ì£¼ì†Œ" columndì•  ê²€ìƒ‰í•˜ë©´ ë¨. 
        df_edge_caseì˜ "ë„ë¡œëª…"ì˜ uniquenessê²€ì‚¬í• ë•ŒëŠ”, "ë„ë¡œëª…" columnì•ˆì—ì„œë§Œ ê²€ìƒ‰í•˜ëŠ”ê²ƒ. 
        
        uniqueí•œ ì• ë“¤ì„ markerid3ì— ê²€ìƒ‰í•˜ê³  ê·¸ ê²°ê³¼ë„ unique í•œ ê²½ìš°, complexNo ê°€ì ¸ì™€ì„œ ê¸°ì…í•˜ê¸°. 
            ê¸°ì… ìœ„ì¹˜ëŠ” df_edge_caseì˜ ì²«ë²ˆì§¸ì—´ì¸ [KEY]markerid ì˜ ì˜†ì¸ ìƒˆë¡œìš´ ì—´ ì¶”ê°€í•˜ê¸°
                ìƒˆë¡œìš´ ì—´ì˜ ì´ë¦„ì€ [P19]markerid
    """
    """
    Match unique ë„ë¡œëª… in df_edge_case to unique [P15]ì£¼ì†Œ in df_markerid3.
    - If ë„ë¡œëª… is duplicated in df_edge_case: label as 'DUPL:edge'
    - If result is duplicated in df_markerid3: label as 'DUPL:markerid3'
    - If no match found: label as 'FAILED2MAP'
    - If unique match found: assign the complexNo
    Returns df_edge_case with new column [P19]markerid inserted after [KEY]markerid.
    """

    df = df_edge_case.copy()

    # Step 1: Identify duplicates
    duplicated_in_edge = set(df["ë„ë¡œëª…"][df["ë„ë¡œëª…"].duplicated(keep=False)])
    duplicated_in_markerid3 = set(
        df_markerid3["[P15]ì£¼ì†Œ"][df_markerid3["[P15]ì£¼ì†Œ"].duplicated(keep=False)]
    )

    # Step 2: Build mapping from [P15]ì£¼ì†Œ â†’ complexNo (only keep unique ones)
    unique_markerid3 = df_markerid3[~df_markerid3["[P15]ì£¼ì†Œ"].isin(duplicated_in_markerid3)]
    address_to_complexNo = dict(zip(unique_markerid3["[P15]ì£¼ì†Œ"], unique_markerid3["complexNo"]))

    # Step 3: Apply logic row-by-row
    results = []
    count_map = {
        "mapped": 0,
        "DUPL:edge": 0,
        "DUPL:markerid3": 0,
        "FAILED2MAP": 0
    }

    for _, row in df.iterrows():
        roadname = row["ë„ë¡œëª…"]

        if roadname in duplicated_in_edge:
            results.append("DUPL:edge")
            count_map["DUPL:edge"] += 1
        elif roadname in duplicated_in_markerid3:
            results.append("DUPL:markerid3")
            count_map["DUPL:markerid3"] += 1
        elif roadname in address_to_complexNo:
            results.append(address_to_complexNo[roadname])
            count_map["mapped"] += 1
        else:
            results.append("FAILED2MAP")
            count_map["FAILED2MAP"] += 1

    # Step 4: Insert [P19]markerid after [KEY]markerid
    insert_index = df.columns.get_loc("[KEY]markerid") + 1
    df.insert(insert_index, "[P19]markerid", results)

    # Step 5: Summary
    total = len(df)
    print("âœ… Summary:")
    print(f"  â€¢ Mapped:         {count_map['mapped']}")
    print(f"  â€¢ DUPL:edge:      {count_map['DUPL:edge']}")
    print(f"  â€¢ DUPL:markerid3: {count_map['DUPL:markerid3']}")
    print(f"  â€¢ FAILED2MAP:     {count_map['FAILED2MAP']}")
    print(f"  â€¢ Total:          {total}")

    return df
    
def preprocess_20(df): # Manual ê²€ìƒ‰ì— í•„ìš”í•œ ì—´ë“¤ì„ 2,3ì—´ë¡œ ì˜®ê¹€. ê²€ìƒì˜ í¸ë¦¬ì„±ì„ ìœ„í•¨. edge case 300ê°œ ì •ë„ ìˆ˜ë™ì‘ì—… í•„ìš”.
    '''
    find the column "ë„ë¡œëª…"
    find the column "[P12]í¬ë¡¤ë§ì¤€ë¹„_ì‹œêµ¬ë‹¨ì§€ëª…"
    
    move both of these columns to the second and third columns respectively. 
    do not overwrite the original 2 and 3 columns
    '''
    
    """
    Move 'ë„ë¡œëª…' and '[P12]í¬ë¡¤ë§ì¤€ë¹„_ì‹œêµ¬ë‹¨ì§€ëª…' columns to the second and third positions,
    without overwriting existing columns.
    """

    df = df.copy()

    # Identify columns to move
    col1 = "ë„ë¡œëª…"
    col2 = "[P12]í¬ë¡¤ë§ì¤€ë¹„_ì‹œêµ¬ë‹¨ì§€ëª…"

    # Remove them temporarily
    cols_to_move = df[[col1, col2]]
    df = df.drop([col1, col2], axis=1)

    # Re-insert them at positions 1 and 2
    df.insert(1, col2, cols_to_move[col2])
    df.insert(1, col1, cols_to_move[col1])

    return df

def preprocess_21(edge_df, step_df): # ìˆ˜ë™ê²€ìƒ‰ ì´í›„ edge_manual.csv ì„ step_15.csvì— ë§¤í•‘í•´ì„œ step_16 ë§Œë“¤ê¸°.
    """
    For each row in step_df, if its [P12]í¬ë¡¤ë§ì¤€ë¹„_ì‹œêµ¬ë‹¨ì§€ëª… exists in edge_df,
    update the [KEY]markerid with the value from edge_df.
    """

    step_df = step_df.copy()

    # Create mapping from edge_manual
    mapping = dict(zip(
        edge_df["[P12]í¬ë¡¤ë§ì¤€ë¹„_ì‹œêµ¬ë‹¨ì§€ëª…"],
        edge_df["[KEY]markerid"]
    ))

    # Condition: if value exists in mapping
    updated_count = 0
    for idx, row in step_df.iterrows():
        key = row["[P12]í¬ë¡¤ë§ì¤€ë¹„_ì‹œêµ¬ë‹¨ì§€ëª…"]
        if key in mapping:
            step_df.at[idx, "[KEY]markerid"] = mapping[key]
            updated_count += 1

    print(f"âœ… Updated {updated_count} rows in step_15 using edge_manual.")

    return step_df










# match_marker_ids_by_regionì™€ ë¹„ìŠ·í•˜ì§€ë§Œ, [MOLIT]ì˜ col"ë„ë¡œëª…"ê³¼ markerid_3ì˜ col"[P15]ì£¼ì†Œ"ê³¼ matchí•¨
    # if: dfì˜ col[P16]matchì˜ ê°’ì´ list ì¸ ê²½ìš° (ìš°ë¦¬ê°€ ìœ„ì—ì„œëŠ” singleì¸ ê²½ìš°ë§Œ í–ˆìŒ! ì´ë²ˆì—ë„ˆëŠ ì œì™¸í–ˆì—ˆë˜ multiple valueìƒí™© ì²˜ë¦¬í• ê±°ì„.)
    # then: í•˜ë‚˜ì”© markerid_3 ì—ì„œ ê²€ìƒ‰í•´ì„œ col[P15]ì£¼ì†Œë¥¼ ë¶ˆëŸ¬ì˜¬ê±°ì„. 
    # ë¶ˆëŸ¬ì˜¨ ì—¬ëŸ¬ê°œì˜ [P15]ì£¼ì†Œë“¤ ì¤‘ì—ì„œ ìš°ë¦¬ ë¦¬ìŠ¤íŠ¸ê°€ ìˆëŠ” rowì˜ col"ë„ë¡œëª…"ì˜ ê°’ì´ë‘ ì¼ì¹˜í•˜ëŠ” idë§Œ ë°˜í™˜í• ê±°ì„. 
    # ë°˜í™˜í•œ single valueëŠ” ìƒˆë¡œìš´ ì—´ë¡œ ì¶”ê°€.(ì—´ ì´ë¦„ì€ [P18]markerid)(ì—´ ìœ„ì¹˜ëŠ” [KEY]markeridì—´ ë°”ë¡œ ë’¤ì—.)

'''
GS - ì§€ì—ìŠ¤
2ë‹¨ì§€ - 2ì°¨
ì•¤ - &
'''

'''
crawl ì—ì„œ ì²˜ëŸ¼ ë˜‘ê°™ì´ step_11 ì˜ [P12]í¬ë¡¤ë§ì¤€ë¹„_ì‹œêµ°êµ¬ë‹¨ì§€ëª… ê²€ìƒ‰í•˜ê¸°.

ì—¬ëŸ¬ ê²°ê³¼ ë‚˜ì˜¤ëŠ”ì§€? ê²°ê³¼ê°€ ì—†ë‹¤ê³  ë‚˜ì˜¤ëŠ”ì§€ íŒë‹¨

if ì—¬ëŸ¬ê²°ê³¼: then get markerid for each result
    í•œ entry ë‹¹ each markerid retrievedë¥¼ markerid_2.csv ì— ê²€ìƒ‰í•´ì„œ ê° sido, gungu, dong, complexName ë¶ˆëŸ¬ì˜¤ê¸°. 
    ok. ê·¸ëŸ¬ë©´ ì—¬ê¸°ê¹Œì§€ ì •ë¦¬í•˜ìë©´, í•˜ë‚˜ì˜ ì£¼ì†Œì— ëŒ€í•´ì„œ ê²€ìƒ‰í–ˆë”ë‹ˆ ì—¬ëŸ¬ê°œê°€ ë‚˜ì™”ëŠ”ë° ì´ì¤‘ ì–´ë–¤ê²ƒì´ ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê±´ì§€ ì •í™•íˆ ì•ˆë‚˜ì˜¨ë‹¤ëŠ” ëœ». 
    ê·¸ë˜ì„œ ì—¬ëŸ¬ ê²€ìƒ‰ ê²°ê³¼ì˜ markeridë¥¼ ê°ê° markerid_2.csv ì—ì„œ ê²€ìƒ‰í•´ì„œ ê·¸ ì‹œêµ°êµ¬ë™ì„ ë¶ˆëŸ¬ì™€ì„œ step_11 ì™€ ì¼ì¹˜í•˜ë©´ ê°€ì ¸ì˜¤ê¸°. ë§Œì•½ ì—¬ëŸ¬ê°œê°€ ì¼ì¹˜í•˜ë©´ SEVERAL ìœ¼ë¡œ í‘œì‹œí•˜ê³ 
    ì˜†ì— ì—´ì—ëŠ” ê·¸ ì‹œêµ°êµ¬ê¹Œì§€ ë§ëŠ” markerid í‘œê¸°í•˜ê¸°.


if ê²°ê³¼ì—†ìŒ: NORESULTë¼ê³  ê·¸ entry ì— í‘œì‹œí•˜ê¸°.

'''